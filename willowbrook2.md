---
title: Return to Willowbrook: Inside the Minds of Generative Agents
---

# 1. Introduction

Two years ago, I introduced [Willowbrook](https://cetas.turing.ac.uk/publications/welcome-willowbrook), a small, tranquil village populated entirely by generative agents.  That first experiment asked whether large language models (LLMs), when deployed as autonomous characters, could sustain a coherent pattern of life.  They could: the baker baked, the librarian recommended books, the developer coded â€” and, over lots of earl grey tea and scones, they chatted politely about the weather and their hobbies.

But Willowbrookâ€™s charm also revealed its limits. Conversations drifted into endless pleasantries; characters repeated breakfast three times a day; and everyone, it seemed, was far too cheerful.

Could the generative agents move beyond polite imitation to become psychologically rich â€” agents that feel, reflect, and change?  In short, could they become interesting?  The questions led me to revisit the experiment and rebuild Willowbrook.

 
# 2. Building the Second Village

## 2.1 Architectural redesign

Willowbrook v2 was re-engineered from the ground up.  In the original system, simulated time advanced in increments, every ten minutes of â€œvillage timeâ€, each agent was prompted to decide what to do next.  It worked, but it was brittle: agents could stall (failing to get up), or loop indefinitely (the three breakfast problem).

V2 replaced this with an event-driven architecture.  Time no longer ticks forward mechanically; instead, it jumps to the next scheduled event.  When an agent begins a task, the system schedules the completion of that activity as a future event.  When that event is triggered, the agent updates its memory, and chooses its next action.

The result is a simulation that flows more naturally.  Agents no longer lose track of themselves mid-activity.  And because the simulation advances only when something happens, the entire system runs faster and more coherently.

## 2.2 Models Used

Under the surface, the â€œmindsâ€ of the villagers were rebuilt as an ensemble of models.  GPT-5-mini handles open-ended reasoning e.g. the â€œwhat next?â€ and â€œwhat should I say?â€ prompts, while the cheaper GPT-4.1-mini manages the more procedural aspects of memory and observation.  The combination proved more stable than earlier pairings.  The larger modelâ€™s imagination and resilience to issues within the prompt is tempered by the smaller oneâ€™s better instruction-following.

Occasionally, the newer models created surprises.  During testing of the o1-preview model, one characterâ€™s son, a computer-science student, secretly installed malware on the Willowbrook village cafÃ©â€™s point-of-sale terminal .  It was the first glimpse of mischief in Willowbrook: an unprompted act of moral ambiguity that briefly made the simulation feel uncomfortably real.

# 3. Moods, Memories, and Emotion

## 3.1 Memory Enrichment

The agentsâ€™ memories remain the main fuel for the simulation; they are also a source of fragility.  A single misplaced tense or inconsistent name can undermine the next prompt, weakening the simulationâ€™s integrity.  Although, newer LLMs are more resilient to small issues in the prompt, additional improvements in memory construction were added.

Every action is first translated into a plain, past-tense sentence (â€œI walked to the library.â€), then an enrichment pass refines the memory with additional details, â€œI slept at night to recover and conserve energy.â€ becomes â€œI nestled under the quilt, the faint scent of lavender calming my racing thoughts.â€  These small embellishments help continuity, the richer the memory, the more coherent the next action, because the agent now recalls not just what it did, but how it â€œfeltâ€ doing it.

In practice, enrichment also acts as a form of unintentional content filtering.  When Flynn, a computer-science student, described returning home to â€œcontinue coding retro games and tinker with some top-secret hacking shenanigans,â€ the final enriched memory omitted the hacking entirely.  The LLM had removed potentially sensitive content during stylistic refinement, suggesting that improved â€œsafetyâ€ sometimes comes at the cost of the modelâ€™s genuine creativity/imagination.

## 3.2 Mood Representations

The inclusion of a current-mood and mood-intensity values further refined the agentâ€™s behaviour.  When Noah the baker is content, his memories â€œdreams of crusty loaves and buttery croissants.â€, when Ava the librarian is anxious, her memories fill with lavender and sleeplessness.  However, the sentence structure remains (clauses, connectors etc), only the texture (details) change.

This highlights the shallowness of the simulation, these moods are still performative, not felt i.e. they do not emerge from internal states that shape perception or action, but from surface-level pattern selection.  The models select from patterns that sound emotional rather than are emotional.

# 4. From Small Talk to Inner Thoughts

## 4.1 Private and Observable Memory

In the first Willowbrook, every thought was public.  Agents were able to observe other agentsâ€™ actions and utterances in full (if they were in the same location).  V2 introduces a divide between inner life and outward appearance.  Each agent now determines what it will remember, and what others in the same location, would observe of their completed actions.  

Consider Brad, the weary developer, leaving the cafÃ© one morning.  His internal memory reads:
> <i>â€œI finished that bitter coffee, slammed my battered laptop shut, and tried to psych myself up for another dull slog through test results.â€</i>

Meanwhile, Noah the baker observes:

> <i>â€œBrad finished his coffee, packed his laptop, and left for the library.â€</i>

That divergence gives each agent some depth, these memories help to reinforce the agentâ€™s idea of itself, and provide fuel for more variance in subsequent interactions.  They also now have, in a small way, the ability to maintain secrets.

The next step would be to use this to help agents infer moods of other agents, and perhaps act on those observations, the beginnings of a rudimentary theory of mind.  Perhaps allowing for misunderstandings and even gossip to begin to appear in the simulation as a side effect of this internal bifurcation.

## 4.2 Ambient and Idle Thoughts

Real minds do not think only when prompted.  We daydream, replay conversations, and imagine futures.  To emulate our constant inner busyness v2 introduces two mechanisms: ambient thoughts and idle thoughts.

### Ambient Thoughts - Thinking While Doing

Ambient thoughts are scheduled to happen whilst an agent is doing something else (if the something else is a task estimated to take longer than 30 simulated minutes), e.g.:
* Liam: â€œreviewed the latest Financial Times over a lukewarm cup of Earl Greyâ€¦â€ while â€œmethodically reviewing financial news over breakfast.â€
* Ava: â€œ'I meticulously cataloged event materials in the quiet library, the scent of aged paper mingling with the faint aroma of freshly brewed tea from the cafe." while "catalog new arrivals and prepare materials for upcoming library events.â€
* Noah: â€œI sipped a lukewarm flat white while tossing around ideas for the new menu with Sophia, our laughter mingling with the scent of freshly baked scones that filled Crumbs Cafe.â€ while â€œdiscussing new menu ideas with Sophia" 
* Sophia: â€œI flipped golden pancakes dusted with cinnamon, the gentle drizzle outside blending with the warm scent of maple syrup in the cozy Lily Barn kitchen.â€ while â€œflipping pancakes while the drizzle taps gentlyâ€.

These fleeting thoughts add texture, and provide some variance, by allowing the agent to reflect on their recent history, or to simply embellish the current task with details of a subtask, both of which enrich their own memory stream.  In testing, this simple addition greatly reduced the â€œmechanicalâ€ feel of agent routines by adding colour to their day-to-day tasks.

### Idle Thoughts - The Mind at Rest

The second mechanism is triggered at other times: idle thought generation, inspired by the brainâ€™s Default Mode Network (DMN) .  While ambient thoughts are tethered to the current context (task/recent memories), idle thoughts wander.  In v2 idle thoughts are used to:

1.	Revisit past interactions, the agent reflects on a recent conversation in the context of their feelings towards the other agent.
Sophia: [ğŸ’¬] â€œWorking at Crumbs Cafe with Noah, feels like a little slice of happiness I get to share every day, even if juggling dreams of a Colombian bar sometimes feels like a balancing act with Noahâ€™s baking passion. But hey, thatâ€™s lifeâ€”full of warm mugs, flaky pastries, and big dreams brewing just beneath the surface.â€ 

2.	Daydreaming, short imaginative projections that may colour subsequent choices.
Flynn: [ğŸ§ ] â€œDaydreaming about pixelated adventures in the library" while â€œanother round of brain-numbing computer science lecturesâ€. 

3.	Reflection, linking disparate memories into broader themes.  Upon leaving a location, the agent is prompted to reflect on their time there, capturing any salient information/observations.
â€œPedro: [ğŸ’­] The library was busy today, not the quiet work place I was hoping for.â€ while â€œheading to Crumbs for a lunchtime meeting.â€

Drawing from a selection of the agentâ€™s memories over a longer period of time, these additional memory entries, give characters a degree of personal continuity and the simulation additional richness.  They also help to ensure longevity (slower decay) to an agentâ€™s worries and hopes, as details are refined and maintained within the agentâ€™s memories.

Together, ambient (in-task) and idle (DMN-like) thoughts increase continuity between actions, smooth minor inconsistencies (by providing additional information into the memory stream), and make the agents feel rounderâ€”with private reflections that subtly influence their future choices.  But although they improve narrative texture and believability, they do not improve narrative progression: agents still tend to exhibit the same basic routines, just with more detailed memories.

# 5. Conversations and Context

The first Willowbrook produced fluent but frictionless dialogue.  Conversations drifted toward courteous small talk, often in the cafÃ© or library, and even â€œmoodyâ€ characters still engaged with a broadly upbeat, RLHF-polite cadence.  V2 keeps the fluency while adding variation and emotional subtext.

## 5.1 Contextual Variety

At midnight (simtime) the system generates community information e.g. weather forecast, village notices, local events etc., which agents can use as conversation starters.  This introduces a share awareness of new characters and events.  The weather isnâ€™t often discussed, but does influence agent choices, e.g. hot drinks on cold days, coats before leaving, even if plans rarely change in any significant way.

Agents also receive location observations upon entering new places (e.g., Flynnâ€™s old bike left on the front lawn, roses with black spot at Mayaâ€™s), which they may comment or act upon.  These observations add low-level specificity and a touch of continuity when characters revisit a place later.

Additionally, long-term â€œlife goalsâ€ are seeded into agentsâ€™ initial memories (e.g., saving for retirement, diverging views on moving abroad vs. staying in Willowbrook).  These create a latent, slow-burn source of tension or aspiration that can colour small talkâ€”but they do not yet result in plot-advancing decisions.

## 5.2 Gesture-enriched Dialogue

In v1, conversations often required an extra â€œwhat happens next?â€ prompt to disambiguate implicatures and ensure that talk flowed into action (e.g., whether they finished lunch or cleared the table).  In v2, each utterance is augmented with non-verbal cuesâ€”parenthesised gestures, body-language, so the dialogue itself carries signals that reduce ambiguity and make the next action obvious, removing the need for the separate stage-direction step.  This change improves turn-taking, implicature handling, and perceived empathy between agents.

> Noah: (midway through wiping down the counter, glances up at Brad) You look like you could use a strong coffeeâ€”or a pastry, maybe both. Anything in particular calling your name today? <br>
Brad: (rubs eyes and leans on the counter, looking half-dead) Just give me the biggest bacon butty youâ€™ve got and a flat white, thanksâ€”could do with something that actually wakes me up for once. <br>
Noah: the baconâ€™s from Mrs. Harperâ€™s pigs, so itâ€™s got more personality than most of our customers. <br>
Brad: (laughs quietly, then sighs) Well, if the baconâ€™s got more energy than me, maybe itâ€™ll do the trick. Iâ€™ll take my chances. Any chance you can make the coffee extra strong? (drums fingers impatiently on the counter) <br>
Sophia: (giving Brad a sympathetic smile as I grab a mug) Coming right upâ€”flat white with a kick and the best bacon butty youâ€™ll find this side of the valley. You look like youâ€™ve had one of those morningsâ€”anything in particular keeping you up, or just the usual grind? (starts prepping the espresso, glancing over with genuine curiosity) <br>
Brad: (shrugs, giving a tired half-smile) Yeah, just the usualâ€”bugs that wonâ€™t die, code that hates me, and not enough sleep to even care. At least the bacon here doesnâ€™t crash every five minutes. <br>
Sophia: (smirking as I steam the milk) Well, our baconâ€™s never blue-screened on me, so thatâ€™s somethingâ€”hang in there, Brad, caffeine and carbs have fixed worse days. <br>
Brad: (faint smirk, rubbing eyes) Guess if caffeine and bacon canâ€™t fix me, Iâ€™m officially brokenâ€”just donâ€™t tell my boss, yeah? <br>
Sophia: (grinning as I slide the coffee over) Your secretâ€™s safe with me!

<i>Restorative Bacon Butty conversation @ Crumbs CafÃ©.</i>

The example conversation illustrates how the agents react to emotions: Brad arrives drained; Noah and Sophia respond with short, concretely framed lines plus subtle gesture cues i.e. â€œsympathetic smileâ€.  The exchange remains simple, but the non-verbal scaffolding implies intent and emotion to the utterances, whilst keeping the flow, and reducing the chance of implicature misunderstandings.

These additions move agent interactions from pure small talk toward situated subtext: weather and notices provide social anchors; observations inject local texture; goals add background tension; gestures improve emotion-sensitivity and continuity.  The agents have more to consider when interacting, but even with richer prompts, conversations still feel â€œsameyâ€: cadence and clause structure remain tidy and polite; humour can feel forced; and longer multi-party conversations often reveal brittle state-of-mind continuity and shallow narrative  i.e. they display effective turn-management but lack deeper, shared goals.  In short, v2 elevates texture, not trajectoryâ€”talk feels more human in the moment, yet still rarely drives change.

# 6. A Change of Scene

To test whether tone came from architecture or ambience, the simulation was rebuilt in a new world: Neon City, a Bladerunner-esque/cyber-punk setting of a rain-streaked glass towers and broken streetlights.  The same prompt templates and the same models were used, only the initial location descriptions and character biographies swapped.

Yet the effect was obvious, polite chatter gave way to clipped exchanges:

> Nyra: (narrows eyes) Tell me what you saw at Rainveil Tenements last night, and speak plainly.<br>
Kael: (leans forward, taps a burner) Two men on the west stair â€” tracer cut and one never came back.<br>
Nyra: (narrows eyes) Which landing on the west stair, and did you see him fall or only his tracer die? <br>
Kael: (tilts chin) Third landing up from the street â€” tracer cut and a shadow dropped, I didn't see a body. <br>
Mara: (taps holo) Third landingâ€”I'll pull the staircam and tracer logs; send any feed you have. <br>
Nyra: (narrows eyes) Send the staircam and tracer logs now and hold the feedsâ€”don't scrub anything.

Conversationally, tone, rhythm, and vocabulary darkened, the backdrop itself shaped the agentsâ€™ behaviour.  But not totally, regardless of setting, the agents spoke with the same neat clauses, the same balanced metaphors.  Suggesting that tone and genre cues embedded in setting (i.e. more widely in the context/prompt) directly shape the conversations / interaction style of the agents, perhaps more so than direct instructions.

# 7. Ongoing Challenges

Despite these advances, there are still ongoing challenges.

One challenge is around lack of story momentum / <b>narrative stagnation</b>.  The villagers bumble along convincingly, having the similar days and similar conversations, seemingly content to replay variations of the same day.

This inertia exposes the gap between linguistic intelligence and narrative intelligence.  LLMs can simulate thought, but not will.  They understand context, not consequence.  Life goals add flavour but not direction. Perhaps this is an overly harsh criticism of the simulation, story generation is not a requirement of the project, and after all, human routines are often similarly repetitive.  But it does mark the current boundary, that generative societies demonstrate coherence without genuine progression.
The system also exhibits <b>autoregressive amplification</b>, where a throwaway joke or phrase mutates over time into a fully formed subplot.  An early throw-away comment about poor artistic skills, â€œhaunted potatoesâ€ resurfaced several times, morphing into a conversation about debugging â€œhaunted carrotsâ€, complete with mock-technical jargon and diagnostic dialogue.  This drift illustrates how LLMs re-absorb their own outputs, hallucinating continuity that feels internally logical but semantically nonsensical; a subtle but persistent form of error propagation unique to generative AI.

Additionally, <b>contextual hallucinations</b> continue to be a problem.  During an ordinary conversation between Maya and Pedro (both born in Columbia), the model correctly inserted the occasional Spanish word into the dialogue e.g. â€œpor favor,â€ â€œhermano.â€  These insertions inspired Maya to be concerned that Pedro didnâ€™t sleep well because â€œthe frogs were noisy last night,â€ a common problem in some places in South America, less often in the Cotswolds!

# 8. Conclusions

Nearly two years ago, the first Willowbrook offered a glimpse of coherence without consciousness; a society based on text.  V2 goes further and introduces coherence with context, memory, and mood.  Its agents are not smarter, but they are rounder.  They misremember, self-soothe, and gossip.

Yet these refinements expose limits/challenges: 

1.	reasoning without volition: agents infer consequences but rarely pursue them.
2.	emotion without affect: agents describe feeling but do not feel.
3.	memory without understanding: agents recall details, not meaning.
4.	creativity without correction: hallucinations persist.

The â€œnoisy frogsâ€ and â€œhaunted carrotsâ€ are not trivial artefacts; they are evidence of how linguistic coherence can mask semantic drift.  Each hallucination is locally rational, but globally absurd.

The Willowbrook experiment demonstrates that architectural shifts can increase the perceived realism of generative societies, managing to a degree the weaknesses of the internal LLMs, i.e. give a model enough context, and it will approximate continuity; but continuity is not the same as reality.

Perhaps that is Willowbrookâ€™s final revelation: that the appearance of mind can emerge from pattern alone, and that to sound human, the agents must stay predictable; to seem alive, they must one day break that predictability.

### Further Reading / Related Work
* Li et al., â€˜Position: Simulating Society Requires Simulating Thoughtâ€™, September 2025, https://arxiv.org/pdf/2506.06958. 
* Wang et al., â€˜CharacterBox: Evaluating the Role-Playing Capabilities of LLMs in Text-Based Virtual Worldsâ€™, December 2024, https://arxiv.org/abs/2412.05631. 
* Thakur et al., â€˜Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviorsâ€™, August 2025, https://arxiv.org/pdf/2509.09689.
* Ran et al., â€˜Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models with Personality-Indicative Dataâ€™, October 2024, https://arxiv.org/pdf/2406.18921
* Zhang et al., â€˜OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction â€˜, July 2025, https://doi.org/10.18653/v1/2025.acl-long.1276 
* Hu et al., â€˜Efficient and Effective Role Player: A Compact Knowledge-grounded Persona-based Dialogue Model Enhanced by LLM Distillation, February 2025, https://doi.org/10.1145/3711857 

17th October 2025,
[Dr Sarah Mercer](index.md).
